{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O--7pLSzEbvG"
      },
      "source": [
        "# Fine-tuning a ceshine/t5-paraphrase-paws-msrp-opinosis model on a detoxification task with prefix\n",
        "Test is performed in collab because I do not have GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Using drive to quckly use my dataset\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz-e60v1jpd3",
        "outputId": "ec3a7132-d487-451e-8553-d1f1ac3a28d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "733f6347-7558-4e7b-9ee1-e6b59153bb99",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:37.592210Z",
          "end_time": "2023-11-03T21:46:47.760124Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "# installing huggingface libraries for dataset, models and metrics\n",
        "!pip install datasets transformers[sentencepiece] sacrebleu\n",
        "!pip install numpy==1.24.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:01.766013Z",
          "iopub.status.busy": "2023-09-24T15:10:01.765366Z",
          "iopub.status.idle": "2023-09-24T15:10:01.772400Z",
          "shell.execute_reply": "2023-09-24T15:10:01.771384Z",
          "shell.execute_reply.started": "2023-09-24T15:10:01.765977Z"
        },
        "trusted": true,
        "id": "dyNd_9UDEbvH",
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:47.759125Z",
          "end_time": "2023-11-03T21:46:53.941043Z"
        }
      },
      "outputs": [],
      "source": [
        "# Necessary inputs\n",
        "import warnings\n",
        "\n",
        "from datasets import load_from_disk, load_metric\n",
        "import transformers\n",
        "# import datasets\n",
        "# import random\n",
        "# import pandas as pd\n",
        "# from IPython.display import display, HTML\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqrWaH_EbvI"
      },
      "source": [
        "## Selecting the model\n",
        "For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:01.774778Z",
          "iopub.status.busy": "2023-09-24T15:10:01.773897Z",
          "iopub.status.idle": "2023-09-24T15:10:01.784448Z",
          "shell.execute_reply": "2023-09-24T15:10:01.783216Z",
          "shell.execute_reply.started": "2023-09-24T15:10:01.774744Z"
        },
        "trusted": true,
        "id": "K425zGK_EbvI",
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:53.941043Z",
          "end_time": "2023-11-03T21:46:53.946794Z"
        }
      },
      "outputs": [],
      "source": [
        "# selecting model checkpoint\n",
        "model_checkpoint = \"ceshine/t5-paraphrase-paws-msrp-opinosis\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:01.787917Z",
          "iopub.status.busy": "2023-09-24T15:10:01.787594Z",
          "iopub.status.idle": "2023-09-24T15:10:03.219266Z",
          "shell.execute_reply": "2023-09-24T15:10:03.218277Z",
          "shell.execute_reply.started": "2023-09-24T15:10:01.787893Z"
        },
        "id": "IreSlFmlIrIm",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:53.947791Z",
          "end_time": "2023-11-03T21:46:59.005287Z"
        }
      },
      "outputs": [],
      "source": [
        "# setting random seed for transformers library\n",
        "transformers.set_seed(42)\n",
        "\n",
        "# Load my dataset from interim/ ditrectory\n",
        "raw_datasets = load_from_disk(\"/content/drive/MyDrive/interim/\")\n",
        "\n",
        "# Load the BLUE metric\n",
        "metric = load_metric(\"sacrebleu\", split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzC6oGOTEbvJ"
      },
      "source": [
        "## Dataset\n",
        "This dataset is exact one from interim/ directory that was formed using my make_dataset.py script. It contains keys `[\"train\", \"validation\", \"test\"]` - which represents a dataset splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.220970Z",
          "iopub.status.busy": "2023-09-24T15:10:03.220604Z",
          "iopub.status.idle": "2023-09-24T15:10:03.229080Z",
          "shell.execute_reply": "2023-09-24T15:10:03.228070Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.220925Z"
        },
        "id": "GWiVUF0jIrIv",
        "outputId": "ecddad9a-8c4e-4b91-a7ad-8f16ba7b002d",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:59.005287Z",
          "end_time": "2023-11-03T21:46:59.006285Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['toxic_comment', 'neutral_comment'],\n",
              "        num_rows: 404443\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['toxic_comment', 'neutral_comment'],\n",
              "        num_rows: 57778\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['toxic_comment', 'neutral_comment'],\n",
              "        num_rows: 115556\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:35:02.763581Z",
          "iopub.status.busy": "2023-09-24T15:35:02.762411Z",
          "iopub.status.idle": "2023-09-24T15:35:02.772979Z",
          "shell.execute_reply": "2023-09-24T15:35:02.771223Z",
          "shell.execute_reply.started": "2023-09-24T15:35:02.763514Z"
        },
        "id": "X6HrpprwIrIz",
        "outputId": "095bac50-431a-400a-a101-2a7b3e92b18e",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:59.005287Z",
          "end_time": "2023-11-03T21:46:59.006285Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'toxic_comment': ['if alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.',\n",
              "  \"now you're getting nasty.\",\n",
              "  'well, we could spare your life, for one.',\n",
              "  \"ah! monkey, you've got to snap out of it.\",\n",
              "  \"i've got orders to put her down.\"],\n",
              " 'neutral_comment': ['if alkar floods her with her mental waste, it would explain the high levels of neurotransmitter.',\n",
              "  \"you're becoming disgusting.\",\n",
              "  'well, we can spare your life.',\n",
              "  'monkey, you have to wake up.',\n",
              "  'i have orders to kill her.']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# samples from train dataset\n",
        "raw_datasets[\"train\"][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data\n",
        "As usual we will need to preprocess data and tokenize it before passing to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.310034Z",
          "iopub.status.busy": "2023-09-24T15:10:03.309664Z",
          "iopub.status.idle": "2023-09-24T15:10:03.505289Z",
          "shell.execute_reply": "2023-09-24T15:10:03.504208Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.310001Z"
        },
        "id": "eXNLu_-nIrJI",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91833a2-0515-44f7-b641-d1126578f444",
        "ExecuteTime": {
          "start_time": "2023-11-03T21:46:59.006285Z",
          "end_time": "2023-11-03T21:47:00.373169Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# we will use autotokenizer for this purpose\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.533132Z",
          "iopub.status.busy": "2023-09-24T15:10:03.532860Z",
          "iopub.status.idle": "2023-09-24T15:10:03.539292Z",
          "shell.execute_reply": "2023-09-24T15:10:03.538145Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.533109Z"
        },
        "trusted": true,
        "id": "LdQIVh2dEbvN",
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:00.397391Z",
          "end_time": "2023-11-03T21:47:00.406020Z"
        }
      },
      "outputs": [],
      "source": [
        "# prefix for model input\n",
        "prefix = \"turn toxic to neutral\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.541473Z",
          "iopub.status.busy": "2023-09-24T15:10:03.540621Z",
          "iopub.status.idle": "2023-09-24T15:10:03.549770Z",
          "shell.execute_reply": "2023-09-24T15:10:03.548874Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.541440Z"
        },
        "id": "vc0BSBLIIrJQ",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:00.405012Z",
          "end_time": "2023-11-03T21:47:00.452844Z"
        }
      },
      "outputs": [],
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "toxic_sent = \"toxic_comment\"\n",
        "target_sent = \"neutral_comment\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + ex for ex in examples[toxic_sent]]\n",
        "    targets = [ex for ex in examples[target_sent]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.551620Z",
          "iopub.status.busy": "2023-09-24T15:10:03.551106Z",
          "iopub.status.idle": "2023-09-24T15:10:03.571019Z",
          "shell.execute_reply": "2023-09-24T15:10:03.569879Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.551587Z"
        },
        "id": "-b70jh26IrJS",
        "outputId": "d4c236ec-9df1-43d3-d140-44c4a02200a0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:00.418663Z",
          "end_time": "2023-11-03T21:47:00.470796Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[919, 12068, 12, 7163, 99, 491, 4031, 19, 18368, 160, 28, 26829, 2670, 6, 24, 3, 9453, 8, 306, 593, 13, 6567, 7031, 1538, 4849, 5, 1], [919, 12068, 12, 7163, 7651, 25, 31, 60, 652, 23147, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[3, 99, 491, 4031, 8347, 7, 160, 28, 160, 2550, 2670, 6, 34, 133, 3209, 8, 306, 1425, 13, 6567, 7031, 1538, 449, 5, 1], [25, 31, 60, 2852, 27635, 53, 5, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# example of preprocessing\n",
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:09:35.662567Z",
          "iopub.status.busy": "2023-09-24T16:09:35.662099Z",
          "iopub.status.idle": "2023-09-24T16:09:37.545512Z",
          "shell.execute_reply": "2023-09-24T16:09:37.544516Z",
          "shell.execute_reply.started": "2023-09-24T16:09:35.662533Z"
        },
        "id": "DDtsaJeVIrJT",
        "outputId": "b16582c0-b5d3-44b4-853e-0045872534c0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:00.432898Z",
          "end_time": "2023-11-03T21:47:00.542652Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'toxic_comment': 'if alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.',\n",
              " 'neutral_comment': 'if alkar floods her with her mental waste, it would explain the high levels of neurotransmitter.',\n",
              " 'input_ids': [919,\n",
              "  12068,\n",
              "  12,\n",
              "  7163,\n",
              "  99,\n",
              "  491,\n",
              "  4031,\n",
              "  19,\n",
              "  18368,\n",
              "  160,\n",
              "  28,\n",
              "  26829,\n",
              "  2670,\n",
              "  6,\n",
              "  24,\n",
              "  3,\n",
              "  9453,\n",
              "  8,\n",
              "  306,\n",
              "  593,\n",
              "  13,\n",
              "  6567,\n",
              "  7031,\n",
              "  1538,\n",
              "  4849,\n",
              "  5,\n",
              "  1],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1],\n",
              " 'labels': [3,\n",
              "  99,\n",
              "  491,\n",
              "  4031,\n",
              "  8347,\n",
              "  7,\n",
              "  160,\n",
              "  28,\n",
              "  160,\n",
              "  2550,\n",
              "  2670,\n",
              "  6,\n",
              "  34,\n",
              "  133,\n",
              "  3209,\n",
              "  8,\n",
              "  306,\n",
              "  1425,\n",
              "  13,\n",
              "  6567,\n",
              "  7031,\n",
              "  1538,\n",
              "  449,\n",
              "  5,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# for the example purpose we will crop the dataset and select first 5000 for train\n",
        "# and 500 for validation and test\n",
        "cropped_datasets = raw_datasets\n",
        "cropped_datasets['train'] = raw_datasets['train'].select(range(5000))\n",
        "cropped_datasets['validation'] = raw_datasets['validation'].select(range(500))\n",
        "cropped_datasets['test'] = raw_datasets['test'].select(range(500))\n",
        "tokenized_datasets = cropped_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:03.605060Z",
          "iopub.status.busy": "2023-09-24T15:10:03.604730Z",
          "iopub.status.idle": "2023-09-24T15:10:04.671893Z",
          "shell.execute_reply": "2023-09-24T15:10:04.670859Z",
          "shell.execute_reply.started": "2023-09-24T15:10:03.605029Z"
        },
        "id": "TlqNaB8jIrJW",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:00.499847Z",
          "end_time": "2023-11-03T21:47:06.508885Z"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "# create a model for the pretrained model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install accelerate -U"
      ],
      "metadata": {
        "id": "GDLntu65If9r",
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:06.510878Z",
          "end_time": "2023-11-03T21:47:06.515276Z"
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.674163Z",
          "iopub.status.busy": "2023-09-24T15:10:04.673474Z",
          "iopub.status.idle": "2023-09-24T15:10:04.681771Z",
          "shell.execute_reply": "2023-09-24T15:10:04.680562Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.674126Z"
        },
        "id": "Bliy8zgjIrJY",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:06.516271Z",
          "end_time": "2023-11-03T21:47:06.547212Z"
        }
      },
      "outputs": [],
      "source": [
        "# defining the parameters for training\n",
        "batch_size = 32\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{toxic_sent}-to-{target_sent}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.00,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    report_to='tensorboard',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.684376Z",
          "iopub.status.busy": "2023-09-24T15:10:04.683883Z",
          "iopub.status.idle": "2023-09-24T15:10:04.693774Z",
          "shell.execute_reply": "2023-09-24T15:10:04.692863Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.684341Z"
        },
        "trusted": true,
        "id": "ypTy8nYUEbvQ",
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:06.530729Z",
          "end_time": "2023-11-03T21:47:06.557175Z"
        }
      },
      "outputs": [],
      "source": [
        "# instead of writing collate_fn function we will use DataCollatorForSeq2Seq\n",
        "# simliarly it implements the batch creation for training\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.695838Z",
          "iopub.status.busy": "2023-09-24T15:10:04.695457Z",
          "iopub.status.idle": "2023-09-24T15:10:04.707222Z",
          "shell.execute_reply": "2023-09-24T15:10:04.706315Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.695806Z"
        },
        "id": "UmvbnJ9JIrJd",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:06.539578Z",
          "end_time": "2023-11-03T21:47:06.558173Z"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# simple postprocessing for text\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "# compute metrics function to pass to trainer\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.709249Z",
          "iopub.status.busy": "2023-09-24T15:10:04.708526Z",
          "iopub.status.idle": "2023-09-24T15:10:04.806768Z",
          "shell.execute_reply": "2023-09-24T15:10:04.805816Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.709216Z"
        },
        "id": "imY1oC3SIrJf",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2023-11-03T21:47:06.546206Z",
          "end_time": "2023-11-03T21:47:06.636354Z"
        }
      },
      "outputs": [],
      "source": [
        "# instead of writing train loop we will use Seq2SeqTrainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:10:04.808506Z",
          "iopub.status.busy": "2023-09-24T15:10:04.808050Z",
          "iopub.status.idle": "2023-09-24T15:18:44.110261Z",
          "shell.execute_reply": "2023-09-24T15:18:44.109150Z",
          "shell.execute_reply.started": "2023-09-24T15:10:04.808459Z"
        },
        "id": "uNx5pyRlIrJh",
        "outputId": "a6d6a550-b446-4a45-a892-143fd9b80624",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1570' max='1570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1570/1570 11:35, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.456775</td>\n",
              "      <td>24.059100</td>\n",
              "      <td>14.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.413215</td>\n",
              "      <td>24.885400</td>\n",
              "      <td>13.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.397439</td>\n",
              "      <td>25.147800</td>\n",
              "      <td>13.924000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.567100</td>\n",
              "      <td>1.392835</td>\n",
              "      <td>25.193300</td>\n",
              "      <td>13.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.567100</td>\n",
              "      <td>1.389202</td>\n",
              "      <td>25.420500</td>\n",
              "      <td>13.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.567100</td>\n",
              "      <td>1.393260</td>\n",
              "      <td>25.513300</td>\n",
              "      <td>13.822000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.310600</td>\n",
              "      <td>1.392417</td>\n",
              "      <td>25.337400</td>\n",
              "      <td>13.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.310600</td>\n",
              "      <td>1.394696</td>\n",
              "      <td>25.874200</td>\n",
              "      <td>13.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.310600</td>\n",
              "      <td>1.395465</td>\n",
              "      <td>25.785900</td>\n",
              "      <td>13.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.219100</td>\n",
              "      <td>1.397373</td>\n",
              "      <td>25.687000</td>\n",
              "      <td>13.768000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1570, training_loss=1.3576157247944243, metrics={'train_runtime': 696.2629, 'train_samples_per_second': 71.812, 'train_steps_per_second': 2.255, 'total_flos': 3068776684707840.0, 'train_loss': 1.3576157247944243, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:19:29.403450Z",
          "iopub.status.busy": "2023-09-24T15:19:29.403061Z",
          "iopub.status.idle": "2023-09-24T15:19:30.003295Z",
          "shell.execute_reply": "2023-09-24T15:19:30.002182Z",
          "shell.execute_reply.started": "2023-09-24T15:19:29.403420Z"
        },
        "trusted": true,
        "id": "FWI1Xp8WEbvS",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# saving model\n",
        "trainer.save_model('best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:19:30.753608Z",
          "iopub.status.busy": "2023-09-24T15:19:30.753167Z",
          "iopub.status.idle": "2023-09-24T15:19:31.676057Z",
          "shell.execute_reply": "2023-09-24T15:19:31.675005Z",
          "shell.execute_reply.started": "2023-09-24T15:19:30.753575Z"
        },
        "trusted": true,
        "id": "F4pKDYxrEbvS",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# loading the model and run inference for it\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('best')\n",
        "model.eval()\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T15:19:31.744595Z",
          "iopub.status.busy": "2023-09-24T15:19:31.744257Z",
          "iopub.status.idle": "2023-09-24T15:19:31.749935Z",
          "shell.execute_reply": "2023-09-24T15:19:31.748926Z",
          "shell.execute_reply.started": "2023-09-24T15:19:31.744568Z"
        },
        "trusted": true,
        "id": "GHzipG6MEbvS",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "def translate(model, inference_request, tokenizer=tokenizer):\n",
        "    input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids=input_ids)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True,temperature=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-24T16:17:24.580846Z",
          "iopub.status.busy": "2023-09-24T16:17:24.580402Z",
          "iopub.status.idle": "2023-09-24T16:17:24.852369Z",
          "shell.execute_reply": "2023-09-24T16:17:24.851186Z",
          "shell.execute_reply.started": "2023-09-24T16:17:24.580812Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWDl7KUTEbvT",
        "outputId": "45ea2d72-3d1f-458b-c92d-d058f4f49222",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now you're a cocky, shit.\n"
          ]
        }
      ],
      "source": [
        "inference_request = prefix + \"Now you're getting cocky, bastard\"\n",
        "translate(model, inference_request,tokenizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}